# Lab03: VGG16 Image Classification - Structured Project

Questo progetto implementa il fine-tuning di VGG16 per classificazione binaria (Cats vs Dogs) seguendo le best practices di strutturazione del codice.

## ğŸ“ Struttura del Progetto

```
polito-aml-project_skeleton/
â”œâ”€â”€ dataset/                  # Dataset classes e utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ custom_dataset.py    # CustomImageDataset + create_annotations_csv
â”œâ”€â”€ models/                   # Architetture modelli
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ vgg_finetuning.py    # VGG16 con fine-tuning
â”œâ”€â”€ utils/                    # Utilities di supporto
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ transforms.py        # Trasformazioni train/val/test
â”‚   â”œâ”€â”€ visualization.py     # Plot e visualizzazioni
â”‚   â””â”€â”€ metrics.py           # Metriche e statistiche
â”œâ”€â”€ checkpoints/             # Modelli salvati durante training
â”œâ”€â”€ data/                    # Dataset scaricato (non in git)
â”œâ”€â”€ train.py                 # Script di training
â”œâ”€â”€ eval.py                  # Script di evaluation
â”œâ”€â”€ requirements.txt         # Dipendenze pip
â””â”€â”€ README.md               # Questo file
```

## ğŸš€ Setup

### 1. Clona il repository
```bash
git clone <your-repo-url>
cd polito-aml-project_skeleton
```

### 2. Installa le dipendenze
```bash
pip install -r requirements.txt
```

### 3. Scarica il dataset
Il dataset Cats vs Dogs puÃ² essere scaricato da Kaggle:
```python
import kagglehub
path = kagglehub.dataset_download("tongpython/cat-and-dog")
```

Poi sposta i file nella cartella `data/`:
```bash
mv <downloaded_path>/* ./data/
```

## ğŸ¯ Training

### Training Base (Feature Extraction)
Allena solo l'ultimo layer con i layer base congelati:

```bash
python train.py \
    --data_dir ./data \
    --epochs 10 \
    --batch_size 128 \
    --lr 0.0001 \
    --freeze_base
```

### Training con Wandb Logging
```bash
python train.py \
    --data_dir ./data \
    --epochs 10 \
    --batch_size 128 \
    --lr 0.0001 \
    --freeze_base \
    --use_wandb \
    --wandb_project "my-vgg16-project"
```

### Parametri Disponibili
- `--data_dir`: Path al dataset (default: `./data`)
- `--epochs`: Numero di epoche (default: `10`)
- `--batch_size`: Batch size (default: `128`)
- `--lr`: Learning rate (default: `0.0001`)
- `--momentum`: Momentum per SGD (default: `0.9`)
- `--val_split`: Percentuale validation set (default: `0.2` = 20%)
- `--freeze_base`: Congela i layer base (feature extraction)
- `--checkpoint_dir`: Directory per i checkpoint (default: `./checkpoints`)
- `--use_wandb`: Abilita logging su Wandb
- `--wandb_project`: Nome progetto Wandb

## ğŸ“Š Evaluation

Valuta un modello salvato sul test set:

```bash
python eval.py \
    --checkpoint ./checkpoints/best_model.pth \
    --data_dir ./data \
    --batch_size 128
```

### Parametri Disponibili
- `--checkpoint`: Path al checkpoint del modello (richiesto)
- `--data_dir`: Path al dataset (default: `./data`)
- `--batch_size`: Batch size (default: `128`)
- `--num_classes`: Numero di classi (default: `2`)

## ğŸ“¦ Moduli

### `dataset/`
- **`custom_dataset.py`**: 
  - `CustomImageDataset`: Dataset PyTorch per caricare immagini da CSV
  - `create_annotations_csv()`: Crea file CSV annotations da struttura cartelle

### `models/`
- **`vgg_finetuning.py`**:
  - `create_vgg16_model()`: Crea VGG16 con pre-trained weights
  - `unfreeze_layers()`: Sblocca N layer per fine-tuning progressivo
  - `count_trainable_parameters()`: Conta parametri trainabili

### `utils/`
- **`transforms.py`**:
  - `get_train_transforms()`: Transforms con data augmentation
  - `get_val_test_transforms()`: Transforms SENZA augmentation
  
- **`visualization.py`**:
  - `denormalize()`: Denormalizza tensor per visualizzazione
  - `visualize_batch()`: Visualizza batch di immagini
  - `plot_training_history()`: Plot loss curves
  
- **`metrics.py`**:
  - `calculate_dataset_stats()`: Statistiche dataset
  - `plot_class_distribution()`: Plot distribuzione classi

## ğŸ“ Dal Notebook al Codice Strutturato

Questo progetto Ã¨ una versione strutturata del notebook `MLVM_lab3_warmup_lab02style.ipynb`.

### Differenze Principali:

**âŒ Notebook (Non Strutturato)**:
- Tutto in un unico file
- Difficile da riutilizzare
- Difficile da testare
- Nessuna modularitÃ 

**âœ… Progetto Strutturato**:
- Codice organizzato in moduli
- Facilmente riutilizzabile
- Command line interface
- Logging con Wandb
- Checkpoint management
- Separazione train/eval

## ğŸ“ˆ Output del Training

Durante il training vedrai:
```
âœ“ Using device: cuda

âœ“ Loading datasets...
  - Training samples: 6400
  - Validation samples: 1600
  - Batch size: 128

âœ“ Creating model...
  - Trainable parameters: 8,194 / 134,268,738
  - Mode: Feature Extraction

============================================================
STARTING TRAINING
============================================================
Epoch [1/10] Train Loss: 0.2345 | Valid Loss: 0.1987 | Valid Acc: 0.9234
  âœ“ New best model saved: ./checkpoints/best_model.pth
...
```

I checkpoint vengono salvati in `./checkpoints/`:
- `checkpoint_epoch_X.pth`: Checkpoint ogni N epoche
- `best_model.pth`: Miglior modello su validation set

## ğŸ”¬ Best Practices Implementate

1. âœ… **ModularitÃ **: Codice diviso in moduli riutilizzabili
2. âœ… **Argparse**: Command line interface professionale
3. âœ… **Data Augmentation**: Solo su training set (IMPORTANTE!)
4. âœ… **Checkpoint Management**: Salvataggio automatico best model
5. âœ… **Logging**: Integrazione Wandb opzionale
6. âœ… **Reproducibility**: requirements.txt + seed management
7. âœ… **Documentation**: Docstrings e README completo
8. âœ… **Lab02 Style**: Seguendo lo stile dei lab precedenti

## ğŸ› Troubleshooting

### Import Errors
Assicurati di essere nella directory root del progetto:
```bash
cd /path/to/polito-aml-project_skeleton
python train.py ...
```

### CUDA Out of Memory
Riduci il batch size:
```bash
python train.py --batch_size 64
```

### Dataset Non Trovato
Verifica che la struttura sia:
```
data/
â”œâ”€â”€ training_set/
â”‚   â””â”€â”€ training_set/
â”‚       â”œâ”€â”€ cats/
â”‚       â””â”€â”€ dogs/
â””â”€â”€ test_set/
    â””â”€â”€ test_set/
        â”œâ”€â”€ cats/
        â””â”€â”€ dogs/
```

## ğŸ“š Riferimenti

- Lab02: Training loop e test function style
- Lab03: Transfer Learning e Fine-tuning VGG16
- [VGG16 Paper](https://arxiv.org/abs/1409.1556)
- [PyTorch Transfer Learning Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)
